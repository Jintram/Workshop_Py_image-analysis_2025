{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56efa91d",
   "metadata": {},
   "source": [
    "# Workshop Python Image Analysis\n",
    "*Martijn Wehrens, September 2025*\n",
    "\n",
    "**Estimated time:** XX mins presenting + YY mins exercises\n",
    "\n",
    "## Chapter 5: A very basic introduction into machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6f976",
   "metadata": {},
   "source": [
    "<font face='Times New Roman' color=red>This notebook will probably need to be run on Google colab or the like, as laptops might not have the right architecture to run this.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8a7a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2f90376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4479]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This code defines a VERY simple model for 12x12 pixels\n",
    "\n",
    "class VerySimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(12*12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "# now test the neural network\n",
    "simpleNN = VerySimpleNN().to('mps')\n",
    "test_data = torch.rand(1, 12, 12, device='mps')  # random 12x12 image\n",
    "pred = simpleNN(test_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.44785488]\n"
     ]
    }
   ],
   "source": [
    "# Now acquire the weights\n",
    "weights = simpleNN.linear.weight.detach().cpu().numpy()\n",
    "bias    = simpleNN.linear.bias.detach().cpu().numpy()\n",
    "# print(weights)\n",
    "# \n",
    "test_data_np = test_data.cpu().numpy().flatten()\n",
    "# print(test_data_np)\n",
    "# And manually calculate the outcome\n",
    "outcome = np.sum(test_data_np*weights[0]) + bias\n",
    "print(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "158441ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_data_np.flatten()\n",
    "# weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1909f682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.3135873)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_data_np.flatten()*weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cd858d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIoAAACICAYAAAA4T3NvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAACFZJREFUeJzt3V9IFFsABvBvV69jhK6EtrakZWAEQQrWLpLBjRYWL4T1ECUR0osQGcQSUZDaQ2AkhJSi0Iv0kvaivoQXWrrJJTUyo4dALKxWbLcM1l2l8s+e++B1u3uz3bM6szN77/eDeXBmds5x/Dh75szM0SSEECCKw6x3BSg1MCgkhUEhKQwKSWFQSAqDQlIYFJLCoJAUBoWkpOtdgX8Lh8OYmppCVlYWTCaT3tX5TxNCIBQKwWazwWyO02YIjbS2topt27YJRVGE3W4Xw8PDUp/zer0CAJckLl6vN+7fRZMWpbu7G263Gx0dHXA4HGhpaYHL5cLY2Bg2b94c87NZWVkAgAr8hnT8okX16G+LWMCfeBA557GYhFD/pqDD4cC+ffvQ2toKYPnrpKCgAOfOncOlS5difjYYDMJiseBXVCHdxKBoaVEs4A/0YWZmBtnZ2TH3Vb0zOz8/j5GRETidzu+FmM1wOp0YHBz8Yf9v374hGAxGLWQ8qgdlenoaS0tLsFqtUeutVit8Pt8P+zc1NcFisUSWgoICtatEKtD98vjy5cuYmZmJLF6vV+8q0SpU78zm5uYiLS0Nfr8/ar3f70d+fv4P+yuKAkVR1K4GqUz1FiUjIwNlZWXweDyRdeFwGB6PB+Xl5WoXR0miyeWx2+1GTU0N9u7dC7vdjpaWFszNzeH06dNaFJeQ36de6F2FhLlspXpXQZugHD9+HJ8+fUJDQwN8Ph9KS0vR39//QweXUodmQ/h1dXWoq6vT6vCUZLpf9VBqYFBICoNCUhgUkmK451HWK97lrxEuNRNlhN+JLQpJYVBICoNCUhgUksKgkBQGhaQwKCQlJcdRYo0rpOI4STzxfqdknA+2KCSFQSEpDApJYVBICoNCUhgUkpKSl8da0fJ2vhEeFVgPtigkhUEhKQwKSWFQSAqDQlIYFJLCoJAU1YNy9epVmEymqGXXrl1qF0NJpsmA2+7du/Hw4cPvhaRzXC/VafIXTE9PX3V2JUpdmvRRxsfHYbPZsGPHDpw8eRLv37//6b6cFTI1qB4Uh8OBzs5O9Pf3o729HRMTEzhw4ABCodCq+3NWyNSgelAqKytx7Ngx7NmzBy6XCw8ePEAgEMD9+/dX3Z+zQqYGzXuZOTk52LlzJ16/fr3qds4KmRo0D8rs7CzevHmDU6dOaV3Uuml5q9/ojxHEo/pXz4ULF/D48WO8ffsWT548wdGjR5GWlobq6mq1i6IkUr1FmZycRHV1NT5//oy8vDxUVFRgaGgIeXl5ahdFSaR6ULq6utQ+JBkA7/WQFAaFpDAoJIVBISn/u9u6Rp0Jwaj1WsEWhaQwKCSFQSEpDApJYVBICoNCUhgUksKgkBQGhaQwKCSFQSEpDApJYVBICoNCUv53jxnEumUfb+ZGLRnhUYJY2KKQFAaFpDAoJIVBISkMCklhUEgKg0JSEh5HGRgYQHNzM0ZGRvDhwwf09PTgyJEjke1CCDQ2NuLOnTsIBALYv38/2tvbUVxcrFql1zMWEuuzRh3LMMJ/5ki4RZmbm0NJSQna2tpW3X7jxg3cunULHR0dGB4exsaNG+FyufD169d1V5b0k3CLUllZicrKylW3CSHQ0tKCK1euoKqqCgBw9+5dWK1W9Pb24sSJE+urLelG1T7KxMQEfD4fnE5nZJ3FYoHD4cDg4OCqn+GskKlB1aD4fD4AgNVqjVpvtVoj2/6Ns0KmBt2vejgrZGpQNSgrs1X7/f6o9X6//6czWSuKguzs7KiFjEfVxwyKioqQn58Pj8eD0tJSAEAwGMTw8DDOnDmjZlE/Fe9SUc9HCdbKCJftCQdldnY2as7YiYkJvHjxAps2bUJhYSHOnz+Pa9euobi4GEVFRaivr4fNZosaa6HUk3BQnj17hoMHD0Z+drvdAICamhp0dnbi4sWLmJubQ21tLQKBACoqKtDf34/MzEz1ak1JZxJCCL0r8U/BYBAWiwW/ogrppl9UPz6/er5bFAv4A32YmZmJ2zfU/aqHUgODQlIYFJJiuKfwV7pMi1gANOg9BUNh9Q+qsUWxoM1xsXxcmW6q4Tqzk5OTHMZPMq/Xi61bt8bcx3BBCYfDmJqaQlZWFkwmE4LBIAoKCuD1ejlqKyGR8yWEQCgUgs1mg9kcuxdiuK8es9m8aro5vJ8Y2fNlsVikjsfOLElhUEiK4YOiKAoaGxv5fwclaXW+DNeZJWMyfItCxsCgkBQGhaQwKCTF8EFpa2vD9u3bkZmZCYfDgadPn+pdJUMYGBjA4cOHYbPZYDKZ0NvbG7VdCIGGhgZs2bIFGzZsgNPpxPj4+JrLM3RQuru74Xa70djYiOfPn6OkpAQulwsfP37Uu2q6S/obm8LA7Ha7OHv2bOTnpaUlYbPZRFNTk461Mh4AoqenJ/JzOBwW+fn5orm5ObIuEAgIRVHEvXv31lSGYVuU+fl5jIyMRL11aDab4XQ6f/rWIS1byxub8Rg2KNPT01haWkrorUNatpY3NuMxbFDIWAwblNzcXKSlpSX01iEtW8sbm/EYNigZGRkoKyuDx+OJrAuHw/B4PCgvL9exZsb3zzc2V6y8sbnmc7feHreWurq6hKIoorOzU7x69UrU1taKnJwc4fP59K6a7kKhkBgdHRWjo6MCgLh586YYHR0V7969E0IIcf36dZGTkyP6+vrEy5cvRVVVlSgqKhJfvnxZU3mGDooQQty+fVsUFhaKjIwMYbfbxdDQkN5VMoRHjx4JLD9+HrXU1NQIIZYvkevr64XVahWKoohDhw6JsbGxNZfHxwxIimH7KGQsDApJYVBICoNCUhgUksKgkBQGhaQwKCSFQSEpDApJYVBICoNCUv4CwdmaFWXVijAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 118.11x118.11 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now define a data class, which actually only produces two pictures, \n",
    "# either a happy or sad emoji, together with annotation\n",
    "\n",
    "# Two image paths\n",
    "img_happy_path = '/Users/m.wehrens/Documents/git_repos/_UVA/2025_teaching_Py_image-analysis/Workshop_Py_image-analysis_2025/images/ML/smile.tif'\n",
    "img_sad_path   = '/Users/m.wehrens/Documents/git_repos/_UVA/2025_teaching_Py_image-analysis/Workshop_Py_image-analysis_2025/images/ML/sad.tif'\n",
    "\n",
    "# Load images\n",
    "img_happy = tiff.imread(img_happy_path)\n",
    "img_sad   = tiff.imread(img_sad_path)\n",
    "\n",
    "# Show one image\n",
    "fig, ax = plt.subplots(1,1, figsize=(3/2.54, 3/2.54))\n",
    "_=ax.imshow(img_happy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef6ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b8f9396",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
